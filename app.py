import streamlit as st
import os
import faiss
import pickle
import pandas as pd
from sentence_transformers import SentenceTransformer
from openai import OpenAI
# import gdown # Google Driveì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ë¡œì§ì€ ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬ ë˜ëŠ” ì œê±°

# ==========================================
# 0. ì„¤ì • (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
# ==========================================
# ì´ ê²½ë¡œëŠ” FAISS ë²¡í„° íŒŒì¼ê³¼ CSV íŒŒì¼ì´ ì €ì¥ëœ ë“œë¼ì´ë¸Œì˜ 'slowproject' í´ë” ê²½ë¡œì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
# Colab í™˜ê²½ì´ ì•„ë‹Œ ì‹¤ì œ ì„œë²„ ë°°í¬ ì‹œì—ëŠ” ì´ ê²½ë¡œë¥¼ ì„œë²„ íŒŒì¼ ì‹œìŠ¤í…œì— ë§ê²Œ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤.
BASE_DIR = "/content/drive/MyDrive/slowproject" # ì˜ˆì‹œ ê²½ë¡œ

# FAISS íŒŒì¼ ë° CSV íŒŒì¼ì´ ì €ì¥ëœ ìƒˆë¡œìš´ í•˜ìœ„ ë””ë ‰í† ë¦¬
DATA_SUBDIR = "slowrecent" # ë°ì´í„°ê°€ ìœ„ì¹˜í•œ í•˜ìœ„ í´ë”ëª…
FAISS_DATA_DIR = "faiss_vectordb_latest" # FAISS ë° CSV íŒŒì¼ì˜ ì‹¤ì œ ê²½ë¡œ

# FAISS ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ
FAISS_INDEX_FILE = os.path.join(FAISS_DATA_DIR, "faiss_index_recent.bin") # ì‚¬ìš©ìê°€ ì§€ì‹œí•œ íŒŒì¼ëª…
FAISS_DOCS_FILE = os.path.join(FAISS_DATA_DIR, "documents.pkl") # FAISS ê´€ë ¨ íŒŒì¼
FAISS_METADATAS_FILE = os.path.join(FAISS_DATA_DIR, "metadata.pkl") # FAISS ê´€ë ¨ íŒŒì¼

# CSV íŒŒì¼ ê²½ë¡œ
CSV_DATA_FILE = os.path.join(FAISS_DATA_DIR, "slowletter_data_recent.csv") # ì˜¤íƒ€ ìˆ˜ì •: RECENT_DATA_DIR -> FAISS_DATA_DIR


# ==========================================
# 1. í™˜ê²½ ë³€ìˆ˜ (OpenAI API KEY)
# ==========================================
API_KEY = os.environ.get("OPENAI_API_KEY")
if not API_KEY:
    st.error("OpenAI API Keyê°€ ì—†ìŠµë‹ˆë‹¤. Streamlit secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ë¡œ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    st.stop() # API í‚¤ê°€ ì—†ìœ¼ë©´ ì•± ì‹¤í–‰ ì¤‘ë‹¨

client = OpenAI(api_key=API_KEY)

# ==========================================
# 2. ë°ì´í„°ì™€ ë²¡í„° ì¸ë±ìŠ¤ ë¡œë“œ (ìºì‹± ì ìš©)
# ==========================================
@st.cache_resource # ìºì‹œ ë°ì½”ë ˆì´í„°: ì•± ì¬ì‹¤í–‰ ì‹œ ë¦¬ì†ŒìŠ¤ ì¬ë¡œë“œ ë°©ì§€
def load_resources():
    """
    ì§€ì •ëœ ê²½ë¡œì—ì„œ FAISS ì¸ë±ìŠ¤, ë¬¸ì„œ, ë©”íƒ€ë°ì´í„° ë° ì „ì²´ CSV DataFrameì„ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    st.info(f"FAISS ì¸ë±ìŠ¤ ë° CSV íŒŒì¼ ë¡œë“œ ì¤‘... (ê²½ë¡œ: {FAISS_DATA_DIR})")

    # FAISS íŒŒì¼ ê²½ë¡œ êµ¬ì„± (ìœ„ì—ì„œ ì •ì˜í•œ ë³€ìˆ˜ ì‚¬ìš©)
    index_path = FAISS_INDEX_FILE
    docs_path = FAISS_DOCS_FILE
    metadatas_path = FAISS_METADATAS_FILE

    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not all(os.path.exists(f) for f in [index_path, docs_path, metadatas_path]):
        st.error(f"FAISS íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”:")
        st.code(f"ì¸ë±ìŠ¤: {index_path}\në¬¸ì„œ: {docs_path}\në©”íƒ€ë°ì´í„°: {metadatas_path}")
        st.info(f"í•´ë‹¹ íŒŒì¼ë“¤ì´ '{DATA_SUBDIR}' í´ë” ë‚´ì— ì •í™•íˆ ìœ„ì¹˜í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop() # íŒŒì¼ì´ ì—†ìœ¼ë©´ ì•± ì‹¤í–‰ ì¤‘ë‹¨

    try:
        faiss_index = faiss.read_index(index_path)
        with open(docs_path, 'rb') as f:
            faiss_documents = pickle.load(f)
        with open(metadatas_path, 'rb') as f:
            faiss_metadatas = pickle.load(f)
        
        # ì¿¼ë¦¬ ì„ë² ë”©ì— ì‚¬ìš©ë  SentenceTransformer ëª¨ë¸ ë¡œë“œ (FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹œ ì‚¬ìš©ëœ ëª¨ë¸ê³¼ ë™ì¼í•´ì•¼ í•¨)
        embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')

    except Exception as e:
        st.error(f"FAISS ì¸ë±ìŠ¤ ë˜ëŠ” ê´€ë ¨ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()

    # CSV DataFrame ë¡œë“œ
    if not os.path.exists(CSV_DATA_FILE):
        st.error(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {CSV_DATA_FILE}")
        st.info(f"'{os.path.basename(CSV_DATA_FILE)}' íŒŒì¼ì´ '{DATA_SUBDIR}' í´ë” ë‚´ì— ì •í™•íˆ ìœ„ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop() # íŒŒì¼ì´ ì—†ìœ¼ë©´ ì•± ì‹¤í–‰ ì¤‘ë‹¨
    
    try:
        df_full = pd.read_csv(CSV_DATA_FILE)
        # 'section_id' ì»¬ëŸ¼ì´ ë¬¸ìì—´ íƒ€ì…ì¸ì§€ í™•ì¸í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ ë§¤ì¹­ ì‹œ ì˜¤ë¥˜ ë°©ì§€
        if 'section_id' in df_full.columns:
            df_full['section_id'] = df_full['section_id'].astype(str)
        else:
            st.warning("ê²½ê³ : ë¡œë“œëœ CSVì— 'section_id' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. FAISS ê²€ìƒ‰ ê²°ê³¼ë¥¼ CSV ë°ì´í„°ì™€ ì •í™•íˆ ë§¤ì¹­í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            st.warning("ë°±ì—”ë“œ ì‹œìŠ¤í…œì—ì„œ 'section_id'ê°€ í¬í•¨ëœ CSVë¥¼ ìƒì„±í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

    except Exception as e:
        st.error(f"CSV íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()

    st.success(f"ëª¨ë“  ë°ì´í„° ë¦¬ì†ŒìŠ¤ ë¡œë“œ ì™„ë£Œ! (ì´ {faiss_index.ntotal:,} ë²¡í„°)")
    return faiss_index, faiss_documents, faiss_metadatas, df_full, embedding_model

# ì „ì—­ì ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ë¡œë“œ (Streamlit ìºì‹œì— ì˜í•´ í•œ ë²ˆë§Œ ì‹¤í–‰ë¨)
faiss_index, faiss_documents, faiss_metadatas, df_full, embedding_model = load_resources()


# ==========================================
# 3. ê²€ìƒ‰ + GPT 2ë‹¨ê³„ ì²˜ë¦¬ (ê¸°ì¡´ RAG ë´‡ ê¸°ëŠ¥)
# ==========================================
def two_pass_rag(query, faiss_index, faiss_documents, faiss_metadatas, df_full, embedding_model, top_k=30):
    # Step 1: ì§ˆë¬¸ ë²¡í„°í™” (SentenceTransformerë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ ì„ë² ë”©)
    try:
        emb = embedding_model.encode([query], normalize_embeddings=True).astype("float32").reshape(1, -1)
    except Exception as e:
        st.error(f"ì¿¼ë¦¬ ì„ë² ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì˜¤ë¥˜: ì¿¼ë¦¬ ì„ë² ë”© ì‹¤íŒ¨."

    # Step 2: 1ì°¨ FAISS ê²€ìƒ‰
    D, I = faiss_index.search(emb, top_k)
    
    retrieved_sections_data = []
    
    for idx in I[0]:
        if 0 <= idx < len(faiss_metadatas): # ìœ íš¨í•œ ì¸ë±ìŠ¤ì¸ì§€ í™•ì¸
            metadata = faiss_metadatas[idx]
            section_id = metadata.get('doc_id')
            
            corresponding_rows = df_full[df_full['section_id'] == section_id]
            
            if not corresponding_rows.empty:
                row_data = corresponding_rows.iloc[0].to_dict()
                retrieved_sections_data.append({
                    'h3_title': row_data.get('h3_title', 'ì œëª© ì—†ìŒ'),
                    'h3_content_text': row_data.get('h3_content_text', 'ë‚´ìš© ì—†ìŒ'),
                    'entities': row_data.get('entities', ''),
                    'events': row_data.get('events', ''),
                    'url': row_data.get('url', ''),
                    'date': row_data.get('date', '') 
                })

    if not retrieved_sections_data:
        return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # Step 3: 1ì°¨ ìš”ì•½ (ë§¥ë½ ì •ë¦¬)
    context = "\n\n".join(
        f"- ì œëª©: {r['h3_title']}\në‚´ìš©: {r['h3_content_text'][:500]}...\nì—”í‹°í‹°:{r['entities']}\nì´ë²¤íŠ¸:{r['events']}\në‚ ì§œ:{r['date'].split('T')[0]}"
        for r in retrieved_sections_data
    )

    summary_prompt = f"""
ë‹¤ìŒì€ ê²€ìƒ‰ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ê¼­ì§€ë“¤ì˜ í•µì‹¬ ë‚´ìš©ì…ë‹ˆë‹¤.
ì´ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ì£¼ìš” ì‚¬ê±´, í•µì‹¬ ì¸ë¬¼, ë°°ê²½ íë¦„**ì„ ì••ì¶•í•´ 500ì ì´ë‚´ë¡œ ìš”ì•½í•˜ì„¸ìš”.
ì—°ë„Â·ì‚¬ëŒÂ·ì‚¬ê±´ì„ ë¹ ì§ì—†ì´ ë‚¨ê¸°ê³ , ì¤‘ë³µì€ í•©ì³ ì£¼ì„¸ìš”.

ê²€ìƒ‰ ë¬¸ì„œ:
{context}
"""
    try:
        summary = client.chat.completions.create(
            model="gpt-4o",
            max_tokens=600,
            messages=[{"role":"user","content":summary_prompt}]
        ).choices[0].message.content
    except Exception as e:
        st.error(f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì˜¤ë¥˜: ìš”ì•½ ìƒì„± ì‹¤íŒ¨."

    # Step 4: ìµœì¢… ë‹µë³€ ìƒì„±
    final_prompt = f"""
ë‹¹ì‹ ì€ 'ìŠ¬ë¡œìš°ë ˆí„°' ìŠ¤íƒ€ì¼ì˜ ì‹¬ì¸µ ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì§ˆë¬¸: {query}

ì•„ë˜ëŠ” ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ ìš”ì•½ì…ë‹ˆë‹¤:
{summary}

# ì‘ì„± ê·œì¹™
- ì²« ì¤„ì— í•µì‹¬ í•œ ë¬¸ì¥ ìš”ì•½
- ì´ì–´ì„œ 5~7ê°œì˜ ë¶ˆë¦¿ í¬ì¸íŠ¸
- ë°œì–¸ì´ë‚˜ ì£¼ì¥ì—ëŠ” ê°€ëŠ¥í•˜ë©´ (ì¸ë¬¼Â·ê¸°ê´€, ë‚ ì§œ ë“±) ì¶œì²˜ë¥¼ ê´„í˜¸ë¡œ ëª…ì‹œí•˜ì„¸ìš”
- ìµœì‹  ê¸°ì‚¬ì¼ìˆ˜ë¡ ë¹„ì¤‘ì„ ë†’ì´ì„¸ìš”
- ê±´ì¡°í•˜ê³  ê°„ê²°í•œ ë¬¸ì²´, ì¸ìš©ì€ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
"""
    try:
        final_answer = client.chat.completions.create(
            model="gpt-4o",
            max_tokens=800,
            messages=[{"role":"user","content":final_prompt}]
        ).choices[0].message.content
    except Exception as e:
        st.error(f"ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì˜¤ë¥˜: ìµœì¢… ë‹µë³€ ìƒì„± ì‹¤íŒ¨."

    return final_answer

# ==========================================
# 4. Streamlit UI
# ==========================================
st.set_page_config(page_title="Slow News Insight Bot", layout="wide") # í˜ì´ì§€ ì œëª© ì„¤ì •
st.sidebar.title("ë©”ë‰´")

# íƒ­ êµ¬ì¡° ì¶”ê°€
tab1, tab2 = st.tabs(["ğŸ’¬ ì§ˆë¬¸í•˜ê¸°", "ğŸ“š ì£¼ì œë³„ íŠ¹ì§‘ í˜ì´ì§€"])

with tab1:
    st.title("Slow News Insight Bot (H3 ê¼­ì§€ ê¸°ë°˜)")
    st.markdown("íŠ¹ì • ì§ˆë¬¸ì— ëŒ€í•´ ê´€ë ¨ëœ ë‰´ìŠ¤ ê¼­ì§€ë¥¼ ê²€ìƒ‰í•˜ê³  ì‹¬ì¸µ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")

    query_text_area = st.text_area("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", key="rag_query_input", height=150)

    if st.button("ë‹µë³€ ìƒì„±", key="rag_execute_button"):
        if query_text_area.strip():
            with st.spinner("ì •ë³´ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                answer = two_pass_rag(query_text_area, faiss_index, faiss_documents, faiss_metadatas, df_full, embedding_model)
                st.write("### ë‹µë³€")
                st.write(answer)
        else:
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

with tab2:
    st.title("ğŸ“š ì£¼ì œë³„ íŠ¹ì§‘ í˜ì´ì§€")
    st.markdown("íŠ¹ì • ì£¼ì œì— ëŒ€í•´ ê´€ë ¨ëœ ë‰´ìŠ¤ ê¼­ì§€ë“¤ì„ ëª¨ì•„ë´…ë‹ˆë‹¤.")

    topic_query = st.text_input("ì£¼ì œ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”:", "ë°˜ë„ì²´ ì‚°ì—… ë™í–¥", key="topic_query_input")
    topic_top_k = st.slider("ì£¼ì œ ê´€ë ¨ ê¼­ì§€ í‘œì‹œ ìˆ˜:", 1, 30, 10, key="topic_top_k_slider")

    if st.button("ì£¼ì œ ê²€ìƒ‰", key="search_topic_button"):
        if topic_query.strip():
            st.subheader(f"'{topic_query}' ì£¼ì œ ê´€ë ¨ ë‰´ìŠ¤ ê¼­ì§€ (ìƒìœ„ {topic_top_k}ê°œ)")
            
            # FAISS ê²€ìƒ‰ ìˆ˜í–‰ (RAGì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ì™€ ìœ ì‚¬)
            try:
                topic_emb = embedding_model.encode([topic_query], normalize_embeddings=True).astype("float32").reshape(1, -1)
                D_topic, I_topic = faiss_index.search(topic_emb, topic_top_k * 2) # ì—¬ìœ  ìˆê²Œ ê²€ìƒ‰
            except Exception as e:
                st.error(f"ì£¼ì œ ê²€ìƒ‰ì„ ìœ„í•œ ì„ë² ë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                st.stop()

            retrieved_topic_sections = []
            for i, idx in enumerate(I_topic[0]):
                if len(retrieved_topic_sections) >= topic_top_k: # ì›í•˜ëŠ” ê°œìˆ˜ë§Œí¼ë§Œ ê°€ì ¸ì˜´
                    break
                if 0 <= idx < len(faiss_metadatas):
                    metadata = faiss_metadatas[idx]
                    section_id = metadata.get('doc_id')
                    
                    corresponding_rows = df_full[df_full['section_id'] == section_id]
                    if not corresponding_rows.empty:
                        row_data = corresponding_rows.iloc[0].to_dict()
                        retrieved_topic_sections.append({
                            'similarity': D_topic[0][i], # ìœ ì‚¬ë„ ì ìˆ˜
                            'h1_title': row_data.get('h1_title', 'ì œëª© ì—†ìŒ'),
                            'h3_title': row_data.get('h3_title', 'ê¼­ì§€ ì œëª© ì—†ìŒ'),
                            'h3_content_text': row_data.get('h3_content_text', 'ë‚´ìš© ì—†ìŒ'),
                            'url': row_data.get('url', '#'),
                            'date': row_data.get('date', ''),
                            'entities': row_data.get('entities', ''),
                            'events': row_data.get('events', '')
                        })
            
            if retrieved_topic_sections:
                for i, section in enumerate(retrieved_topic_sections):
                    st.markdown(f"**{i+1}. {section['h3_title']}** (ìœ ì‚¬ë„: {section['similarity']:.4f})")
                    if section['h1_title']:
                        st.markdown(f"**ëŒ€ì£¼ì œ (H1):** {section['h1_title']}")
                    st.markdown(f"**ê²Œì‹œì¼:** {section['date'].split('T')[0]}")
                    st.markdown(f"**URL:** [ë§í¬]({section['url']})")
                    st.markdown(f"**ì—”í‹°í‹°:** {section['entities']}")
                    st.markdown(f"**ì‚¬ê±´:** {section['events']}")
                    with st.expander("ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°"):
                        st.write(section['h3_content_text'])
                    st.markdown("---")
            else:
                st.info("í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ëœ ê¼­ì§€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

            # (ì„ íƒ ì‚¬í•­) ì£¼ì œ ìš”ì•½ ìƒì„± ë²„íŠ¼
            if retrieved_topic_sections and st.button("ì´ ì£¼ì œ ìš”ì•½í•˜ê¸°", key="summarize_topic_button"):
                with st.spinner("ì£¼ì œ ìš”ì•½ ìƒì„± ì¤‘..."):
                    context_for_summary = "\n\n".join(
                        f"- ì œëª©: {r['h3_title']}\në‚´ìš©: {r['h3_content_text'][:500]}...\nì—”í‹°í‹°:{r['entities']}\nì´ë²¤íŠ¸:{r['events']}\në‚ ì§œ:{r['date'].split('T')[0]}"
                        for r in retrieved_topic_sections
                    )
                    
                    topic_summary_prompt = f"""
                    ë‹¤ìŒì€ '{topic_query}' ì£¼ì œì™€ ê´€ë ¨ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ê¼­ì§€ë“¤ì˜ í•µì‹¬ ë‚´ìš©ì…ë‹ˆë‹¤.
                    ì´ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹ ì£¼ì œì˜ **ì£¼ìš” ì‚¬ê±´, í•µì‹¬ ì¸ë¬¼, ë°°ê²½ íë¦„**ì„ ì••ì¶•í•´ 500ì ì´ë‚´ë¡œ ìš”ì•½í•˜ê³ ,
                    'ìŠ¬ë¡œìš°ë ˆí„°' ìŠ¤íƒ€ì¼ì˜ íŠ¹ì§‘ í˜ì´ì§€ ë„ì…ë¶€ì²˜ëŸ¼ ì‘ì„±í•˜ì„¸ìš”.

                    ê²€ìƒ‰ ë¬¸ì„œ:
                    {context_for_summary}
                    """
                    try:
                        topic_summary = client.chat.completions.create(
                            model="gpt-4o",
                            max_tokens=600,
                            messages=[{"role":"user","content":topic_summary_prompt}]
                        ).choices[0].message.content
                        st.subheader("ğŸ’¡ ì£¼ì œ ìš”ì•½")
                        st.write(topic_summary)
                    except Exception as e:
                        st.error(f"ì£¼ì œ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        else:
            st.warning("ì£¼ì œ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

st.markdown("---")
st.caption("FAISS ê²€ìƒ‰ ì‹œìŠ¤í…œ by Gemini AI")
